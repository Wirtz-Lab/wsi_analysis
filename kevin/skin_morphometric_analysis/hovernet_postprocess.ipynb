{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### This part is hovernet preprocess: After MATLAB part of analysis finished on the MATLAB code, this is for modifying some of the wsi(ndpi)src, roisrc, and ndpisrc so that we're only using the 214 image files that we want to use. The MATLAB all_output_concat was manually made by just copy and pasting contents of the each of the excel output files from the MATLAB workflow. After running hovernet workflow, manually add hovernet result to all_output_concat to match 1090 features.\n",
    "### These src's are used in the hovernet_json2df.py, just making sure all the src's are correct so that hovernet_json2df.py runs properly!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xlsrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\CLUE_image_list_230207_v2.xlsx'\n",
    "ndpisrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi'\n",
    "xl = pd.read_excel(xlsrc)\n",
    "xl_tmp = xl[xl[\"student score\"] > 1]\n",
    "wsi_list = xl_tmp.filename\n",
    "# create ndpi path:\n",
    "wsi_ndpi_list = [os.path.join(ndpisrc, x) for x in wsi_list]\n",
    "dst_src = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\desired_wsi'\n",
    "for filename in wsi_ndpi_list:\n",
    "    shutil.copy(filename, dst_src)\n",
    "# create dl path:\n",
    "dlsrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\DLmask1um'\n",
    "wsi_list = [x.replace(\".ndpi\", \".tif\") for x in wsi_list]\n",
    "wsi_dl_list = [os.path.join(dlsrc, x) for x in wsi_list]\n",
    "dst_src1 = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\DLmask1um\\desired_DLmask'\n",
    "for filename in wsi_dl_list:\n",
    "    shutil.copy(filename, dst_src1)\n",
    "# create roi path:\n",
    "roisrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\annotations\\roi\\labeledmask_v2_021723'\n",
    "wsi_list = [x.replace(\".tif\", \".png\") for x in wsi_list]\n",
    "wsi_roi_list = [os.path.join(roisrc, x) for x in wsi_list]\n",
    "dst_src2 = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\annotations\\roi\\labeledmask_v2_021723\\desired_roi'\n",
    "for filename in wsi_roi_list:\n",
    "    shutil.copy(filename, dst_src2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This part is hovernet postprocess: This code belows processes the generated .pkl file from hovernet to the .xlsx file in a form that we desire:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "pkl_src=r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df'\n",
    "output_src=r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel'\n",
    "pkl_list = [x for x in os.listdir(pkl_src) if x.endswith(\".pkl\")]\n",
    "pkl_full_list = [os.path.join(pkl_src,x) for x in pkl_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# to make all_df:\n",
    "# with open(pkl_full_list[1], 'rb') as f:\n",
    "#     print(\"converting pkl to df:\")\n",
    "#     df = pickle.load(f)\n",
    "#     bbox_tmp = df[\"bbox\"].tolist()\n",
    "#     for idx1 in range(len(bbox_tmp)):\n",
    "#         bbox_tmp[idx1] = [[int(x/2),int(y/2)] for (x,y) in bbox_tmp[idx1]]\n",
    "#     df[\"bbox_20x\"] = bbox_tmp\n",
    "#     df['centroid_20x'] = df['centroid'].apply(lambda row: [_/2 for _ in row])\n",
    "#\n",
    "#     contour_tmp = df[\"contour\"].tolist()\n",
    "#     for idx2 in range(len(contour_tmp)):\n",
    "#         contour_tmp[idx2] = [[int(x/2),int(y/2)] for (x,y) in contour_tmp[idx2]]\n",
    "#     df[\"contour_20x\"] = contour_tmp\n",
    "# # df dict, key is the type and the value is a DataFrame with the relevant rows\n",
    "# dfs_by_type = {}\n",
    "# for t in range(2, 12):\n",
    "#     dfs_by_type[t] = df[df['type'] == t].reset_index(drop=True)\n",
    "#\n",
    "# expanded_dfs = []\n",
    "# for t, df_t in dfs_by_type.items():\n",
    "#     new_cols = {f\"{col}_type{t}\": df_t[col] for col in df_t.columns if col != 'type'}\n",
    "#     expanded_dfs.append(pd.DataFrame(new_cols))\n",
    "#\n",
    "# expanded_df = pd.concat(expanded_dfs, axis=1)\n",
    "#\n",
    "# c2t_distance_cols = expanded_df.filter(regex='c2t_distance').columns\n",
    "# col_name_list = list()\n",
    "# for col in c2t_distance_cols:\n",
    "#     for i in range(2, 13):\n",
    "#         col_name = str(col) + \"_\" + str(i)\n",
    "#         col_name_list.append(col_name)\n",
    "# all_df = pd.DataFrame(columns=col_name_list)\n",
    "# all_df.to_excel(r\"C:\\Users\\Kevin\\Desktop\\test.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pickle File Processed:   0%|\u001B[31m          \u001B[0m| 0/214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting pkl to df:\n",
      "extracting distance\n",
      "empty row number 4015\n",
      "processing column: c2t_distance_type2\n",
      "processing column: c2t_distance_type3\n",
      "processing column: c2t_distance_type4\n",
      "processing column: c2t_distance_type5\n",
      "processing column: c2t_distance_type6\n",
      "processing column: c2t_distance_type7\n",
      "processing column: c2t_distance_type8\n",
      "processing column: c2t_distance_type9\n",
      "processing column: c2t_distance_type10\n",
      "processing column: c2t_distance_type11\n",
      "calculating average\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\AppData\\Local\\Temp\\ipykernel_21568\\4055628714.py:69: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  groupdf = pd.DataFrame(grouped_list[idx4].mean(skipna=True)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of avg df is (1, 261)\n",
      "saving excel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pickle File Processed:   0%|\u001B[31m          \u001B[0m| 1/214 [00:23<1:23:08, 23.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting pkl to df:\n"
     ]
    }
   ],
   "source": [
    "for idx, pkl in tqdm(enumerate(range(len(pkl_list))),desc=\"Pickle File Processed\",colour='red',total = len(pkl_list)):\n",
    "    with open(pkl_full_list[idx], 'rb') as f:\n",
    "        print(\"converting pkl to df:\")\n",
    "        df = pickle.load(f)\n",
    "        bbox_tmp = df[\"bbox\"].tolist()\n",
    "        for idx1 in range(len(bbox_tmp)):\n",
    "            bbox_tmp[idx1] = [[int(x/2),int(y/2)] for (x,y) in bbox_tmp[idx1]]\n",
    "        df[\"bbox_20x\"] = bbox_tmp\n",
    "        df['centroid_20x'] = df['centroid'].apply(lambda row: [_/2 for _ in row])\n",
    "\n",
    "        contour_tmp = df[\"contour\"].tolist()\n",
    "        for idx2 in range(len(contour_tmp)):\n",
    "            contour_tmp[idx2] = [[int(x/2),int(y/2)] for (x,y) in contour_tmp[idx2]]\n",
    "        df[\"contour_20x\"] = contour_tmp\n",
    "    # df dict, key is the type and the value is a DataFrame with the relevant rows\n",
    "    dfs_by_type = {}\n",
    "    for t in range(2, 12):\n",
    "        dfs_by_type[t] = df[df['type'] == t].reset_index(drop=True)\n",
    "\n",
    "    expanded_dfs = []\n",
    "    for tt, df_t in dfs_by_type.items():\n",
    "        new_cols = {f\"{col}_type{tt}\": df_t[col] for col in df_t.columns if col != 'type'}\n",
    "        expanded_dfs.append(pd.DataFrame(new_cols))\n",
    "\n",
    "    expanded_df = pd.concat(expanded_dfs, axis=1)\n",
    "    imID,_ = os.path.splitext(pkl_list[idx])\n",
    "    output_pth = os.path.join(output_src,imID + \".xlsx\")\n",
    "    # save original df\n",
    "    expanded_df.to_excel(output_pth, index=False)\n",
    "\n",
    "    # extract the distances:\n",
    "    print(\"extracting distance\")\n",
    "    all_df = pd.read_excel(r\"C:\\Users\\Kevin\\Desktop\\test.xlsx\")\n",
    "    del all_df[all_df.columns[0]]\n",
    "    all_df = all_df.reindex(range(expanded_df.shape[0]))\n",
    "    print(\"empty row number\", expanded_df.shape[0])\n",
    "    c2t_distance_cols = expanded_df.filter(regex='c2t_distance').columns\n",
    "    for _, col1 in enumerate(c2t_distance_cols):\n",
    "        print(\"processing column: {}\".format(col1))\n",
    "        tmp_df = expanded_df[[col1]]\n",
    "        colnames = [col1 + \"_\" + str(kk) for kk in range(2,13)]\n",
    "        for idid, values in tmp_df.iterrows():\n",
    "            if np.any(pd.isnull(values)):\n",
    "                continue\n",
    "            for idx3, value in enumerate(values[0]):\n",
    "                all_df.loc[idid,colnames[idx3]] = value\n",
    "    expanded_df = pd.concat([expanded_df,all_df],axis=1)\n",
    "\n",
    "    for col2 in expanded_df.columns:\n",
    "        if expanded_df[col2].dtype not in [int, float]:\n",
    "            if col2 == \"inroi_type2\":\n",
    "                continue\n",
    "            if \"c2t_distance_type\" in col2:\n",
    "                continue\n",
    "            expanded_df.drop(columns=col2,inplace=True)\n",
    "\n",
    "    # drop cols with inroi in them\n",
    "    inroi_cols = [col3 for col3 in expanded_df.columns if 'inroi_type' in col3]\n",
    "    inroi_cols = inroi_cols[1:] # keep inroi_type2\n",
    "    expanded_df = expanded_df.drop(columns=inroi_cols)\n",
    "    print(\"calculating average\")\n",
    "    # groupby and average now:\n",
    "    expanded_df = expanded_df.replace([np.inf, -np.inf, np.nan], np.nan)\n",
    "    grouped = expanded_df.groupby('inroi_type2')\n",
    "    grouped_list = [grouped.get_group(x) for x in grouped.groups]\n",
    "\n",
    "    finaldf = pd.DataFrame()\n",
    "    for idx4 in range(len(grouped_list)): #\n",
    "        groupdf = pd.DataFrame(grouped_list[idx4].mean(skipna=True)).T\n",
    "        finaldf = pd.concat([finaldf,groupdf])\n",
    "    print(\"shape of avg df is\",finaldf.shape)\n",
    "\n",
    "    # save average df\n",
    "    print(\"saving excel\")\n",
    "    with pd.ExcelWriter(output_pth, engine='openpyxl', mode='a') as writer:\n",
    "        finaldf.to_excel(writer, sheet_name='averages', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Then calculate CV of all of the values to complete 1090 features:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
