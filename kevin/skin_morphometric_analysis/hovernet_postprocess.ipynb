{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### This part is hovernet preprocess: After MATLAB part of analysis finished on the MATLAB code, this is for modifying some of the wsi(ndpi)src, roisrc, and ndpisrc so that we're only using the 214 image files that we want to use. The MATLAB all_output_concat was manually made by just copy and pasting contents of the each of the excel output files from the MATLAB workflow. After running hovernet workflow, manually add hovernet result to all_output_concat to match 1090 features.\n",
    "### These src's are used in the hovernet_json2df.py, just making sure all the src's are correct so that hovernet_json2df.py runs properly!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xlsrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\CLUE_image_list_230207_v2.xlsx'\n",
    "ndpisrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi'\n",
    "xl = pd.read_excel(xlsrc)\n",
    "xl_tmp = xl[xl[\"student score\"] > 1]\n",
    "wsi_list = xl_tmp.filename\n",
    "# create ndpi path:\n",
    "wsi_ndpi_list = [os.path.join(ndpisrc, x) for x in wsi_list]\n",
    "dst_src = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\desired_wsi'\n",
    "for filename in wsi_ndpi_list:\n",
    "    shutil.copy(filename, dst_src)\n",
    "# create dl path:\n",
    "dlsrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\DLmask1um'\n",
    "wsi_list = [x.replace(\".ndpi\", \".tif\") for x in wsi_list]\n",
    "wsi_dl_list = [os.path.join(dlsrc, x) for x in wsi_list]\n",
    "dst_src1 = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\DLmask1um\\desired_DLmask'\n",
    "for filename in wsi_dl_list:\n",
    "    shutil.copy(filename, dst_src1)\n",
    "# create roi path:\n",
    "roisrc = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\annotations\\roi\\labeledmask_v2_021723'\n",
    "wsi_list = [x.replace(\".tif\", \".png\") for x in wsi_list]\n",
    "wsi_roi_list = [os.path.join(roisrc, x) for x in wsi_list]\n",
    "dst_src2 = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\annotations\\roi\\labeledmask_v2_021723\\desired_roi'\n",
    "for filename in wsi_roi_list:\n",
    "    shutil.copy(filename, dst_src2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This part is hovernet postprocess: This code belows processes the generated .pkl file from hovernet to the .xlsx file in a form that we desire (the average values over each patient's roi):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "pkl_src=r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df'\n",
    "output_src=r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel'\n",
    "pkl_list = [x for x in os.listdir(pkl_src) if x.endswith(\".pkl\")]\n",
    "pkl_full_list = [os.path.join(pkl_src,x) for x in pkl_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# # Code to make the test.xlsx loaded in below: (It's just empty df with column names)\n",
    "# with open(pkl_full_list[1], 'rb') as f:\n",
    "#     print(\"converting pkl to df:\")\n",
    "#     df = pickle.load(f)\n",
    "#     bbox_tmp = df[\"bbox\"].tolist()\n",
    "#     for idx1 in range(len(bbox_tmp)):\n",
    "#         bbox_tmp[idx1] = [[int(x/2),int(y/2)] for (x,y) in bbox_tmp[idx1]]\n",
    "#     df[\"bbox_20x\"] = bbox_tmp\n",
    "#     df['centroid_20x'] = df['centroid'].apply(lambda row: [_/2 for _ in row])\n",
    "#\n",
    "#     contour_tmp = df[\"contour\"].tolist()\n",
    "#     for idx2 in range(len(contour_tmp)):\n",
    "#         contour_tmp[idx2] = [[int(x/2),int(y/2)] for (x,y) in contour_tmp[idx2]]\n",
    "#     df[\"contour_20x\"] = contour_tmp\n",
    "# # df dict, key is the type and the value is a DataFrame with the relevant rows\n",
    "# dfs_by_type = {}\n",
    "# for t in range(2, 12):\n",
    "#     dfs_by_type[t] = df[df['type'] == t].reset_index(drop=True)\n",
    "#\n",
    "# expanded_dfs = []\n",
    "# for t, df_t in dfs_by_type.items():\n",
    "#     new_cols = {f\"{col}_type{t}\": df_t[col] for col in df_t.columns if col != 'type'}\n",
    "#     expanded_dfs.append(pd.DataFrame(new_cols))\n",
    "#\n",
    "# expanded_df = pd.concat(expanded_dfs, axis=1)\n",
    "#\n",
    "# c2t_distance_cols = expanded_df.filter(regex='c2t_distance').columns\n",
    "# col_name_list = list()\n",
    "# for col in c2t_distance_cols:\n",
    "#     for i in range(2, 13):\n",
    "#         col_name = str(col) + \"_\" + str(i)\n",
    "#         col_name_list.append(col_name)\n",
    "# all_df = pd.DataFrame(columns=col_name_list)\n",
    "# all_df.to_excel(r\"C:\\Users\\Kevin\\Desktop\\test.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pickle File Processed:   0%|\u001B[31m          \u001B[0m| 0/214 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting pkl to df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pickle File Processed:   0%|\u001B[31m          \u001B[0m| 0/214 [00:10<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, pkl in tqdm(enumerate(range(len(pkl_list))),desc=\"Pickle File Processed\",colour='red',total = len(pkl_list)):\n",
    "    with open(pkl_full_list[idx], 'rb') as f:\n",
    "        print(\"converting pkl to df:\")\n",
    "        df = pickle.load(f)\n",
    "        bbox_tmp = df[\"bbox\"].tolist()\n",
    "        for idx1 in range(len(bbox_tmp)):\n",
    "            bbox_tmp[idx1] = [[int(x/2),int(y/2)] for (x,y) in bbox_tmp[idx1]]\n",
    "        df[\"bbox_20x\"] = bbox_tmp\n",
    "        df['centroid_20x'] = df['centroid'].apply(lambda row: [_/2 for _ in row])\n",
    "\n",
    "        contour_tmp = df[\"contour\"].tolist()\n",
    "        for idx2 in range(len(contour_tmp)):\n",
    "            contour_tmp[idx2] = [[int(x/2),int(y/2)] for (x,y) in contour_tmp[idx2]]\n",
    "        df[\"contour_20x\"] = contour_tmp\n",
    "    # df dict, key is the type and the value is a DataFrame with the relevant rows\n",
    "    dfs_by_type = {}\n",
    "    for t in range(2, 12):\n",
    "        dfs_by_type[t] = df[df['type'] == t].reset_index(drop=True)\n",
    "\n",
    "    expanded_dfs = []\n",
    "    for tt, df_t in dfs_by_type.items():\n",
    "        new_cols = {f\"{col}_type{tt}\": df_t[col] for col in df_t.columns if col != 'type'}\n",
    "        expanded_dfs.append(pd.DataFrame(new_cols))\n",
    "\n",
    "    expanded_df = pd.concat(expanded_dfs, axis=1)\n",
    "    imID,_ = os.path.splitext(pkl_list[idx])\n",
    "    output_pth = os.path.join(output_src,imID + \".xlsx\")\n",
    "\n",
    "    # save original df\n",
    "    expanded_df.to_excel(output_pth, index=False)\n",
    "\n",
    "    # extract the distances:\n",
    "    print(\"extracting distance\")\n",
    "    all_df = pd.read_excel(r\"C:\\Users\\Kevin\\Desktop\\test.xlsx\")\n",
    "    del all_df[all_df.columns[0]]\n",
    "    all_df = all_df.reindex(range(expanded_df.shape[0]))\n",
    "    print(\"empty row number\", expanded_df.shape[0])\n",
    "    c2t_distance_cols = expanded_df.filter(regex='c2t_distance').columns\n",
    "    for _, col1 in enumerate(c2t_distance_cols):\n",
    "        print(\"processing column: {}\".format(col1))\n",
    "        tmp_df = expanded_df[[col1]]\n",
    "        colnames = [col1 + \"_\" + str(kk) for kk in range(2,13)] # matches the colnames of the all_df\n",
    "        for idid, values in tmp_df.iterrows():\n",
    "            if np.any(pd.isnull(values)):\n",
    "                continue\n",
    "            for idx3, value in enumerate(values[0]):\n",
    "                all_df.loc[idid,colnames[idx3]] = value\n",
    "    expanded_df = pd.concat([expanded_df,all_df],axis=1)\n",
    "    print(\"expanded_df size after extracting distance\", expanded_df.shape)\n",
    "\n",
    "    for col2 in expanded_df.columns:\n",
    "        if expanded_df[col2].dtype not in [int, float]:\n",
    "            if col2 == \"inroi_type2\":\n",
    "                continue\n",
    "            if \"c2t_distance_type\" in col2:\n",
    "                continue\n",
    "            expanded_df.drop(columns=col2,inplace=True)\n",
    "    print(\"expanded_df size after dropping some columns\", expanded_df.shape)\n",
    "    # drop cols with inroi in them\n",
    "    inroi_cols = [col3 for col3 in expanded_df.columns if 'inroi_type' in col3]\n",
    "    inroi_cols = inroi_cols[1:] # keep inroi_type2\n",
    "    expanded_df = expanded_df.drop(columns=inroi_cols)\n",
    "    for col3 in expanded_df.columns:\n",
    "        if col3 in \"c2t_distance_type\" and len(col3.split(\"_\")) == 3: #drop the original c2t_distance_types, but keep the c2t_distance_type2_2.\n",
    "            expanded_df.drop(columns=col3,inplace=True)\n",
    "    print(\"expanded_df size after dropping some columns2\", expanded_df.shape)\n",
    "    print(\"calculating average\")\n",
    "    # groupby and average now:\n",
    "    expanded_df = expanded_df.replace([np.inf, -np.inf, np.nan], np.nan) # replace inf -inf and nan wit han's\n",
    "    print(\"expanded_df size after dropping nan's\", expanded_df.shape)\n",
    "    grouped = expanded_df.groupby('inroi_type2')\n",
    "    grouped_list = [grouped.get_group(x) for x in grouped.groups]\n",
    "\n",
    "    finalavgdf = pd.DataFrame()\n",
    "    finalstddf = pd.DataFrame()\n",
    "    for idx4 in range(len(grouped_list)): #\n",
    "        groupavgdf = pd.DataFrame(grouped_list[idx4].mean(skipna=True)).T #nanmean\n",
    "        finalavgdf = pd.concat([finalavgdf,groupavgdf])\n",
    "        groupstddf = pd.DataFrame(grouped_list[idx4].std(skipna=True)).T #nanstd\n",
    "        finalstddf = pd.concat([finalstddf,groupstddf])\n",
    "    print(\"shape of avg df is\",finalavgdf.shape)\n",
    "    print(\"shape of std df is\",finalstddf.shape)\n",
    "\n",
    "    # save average df\n",
    "    print(\"saving excel\")\n",
    "    with pd.ExcelWriter(output_pth, engine='openpyxl', mode='a') as writer:\n",
    "        finalavgdf.to_excel(writer, sheet_name='averages', index=False)\n",
    "        finalstddf.to_excel(writer, sheet_name='std', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now merge all 214 excel sheets of average together to create a one, large dataframe that we can use to concatenate to the overall clue data df. Note that not all of the excel sheets have the same # of columns, since columns with all NAN's get dropped. Just merge by row!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Excel File Processed: 100%|\u001B[31m██████████\u001B[0m| 214/214 [00:07<00:00, 29.67it/s]\n"
     ]
    }
   ],
   "source": [
    "ind_xl_src = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel'\n",
    "ind_xl_name = [x for x in os.listdir(ind_xl_src) if x.endswith(\".xlsx\")]\n",
    "ind_xl_path = [os.path.join(ind_xl_src,y) for y in ind_xl_name]\n",
    "total_df = pd.DataFrame()\n",
    "for _, xlpth in tqdm(enumerate(ind_xl_path),desc=\"Excel File Processed\",colour='red',total = len(ind_xl_path)):\n",
    "    ind_df = pd.read_excel(xlpth,sheet_name='averages')\n",
    "    total_df = pd.concat([total_df,ind_df],axis=0)\n",
    "total_df = total_df.reset_index(drop=True)\n",
    "save_path = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel\\total_df'\n",
    "total_df.to_excel(os.path.join(save_path,\"total_avg\" + \".xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Then merge all 214 excel sheets of std together to create one, large dataframe we can use to concatenate to th eoverall clue data df."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Excel File Processed: 100%|\u001B[31m██████████\u001B[0m| 214/214 [00:09<00:00, 21.70it/s]\n"
     ]
    }
   ],
   "source": [
    "ind_xl_src = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel'\n",
    "ind_xl_name = [x for x in os.listdir(ind_xl_src) if x.endswith(\".xlsx\")]\n",
    "ind_xl_path = [os.path.join(ind_xl_src,y) for y in ind_xl_name]\n",
    "total_df = pd.DataFrame()\n",
    "for _, xlpth1 in tqdm(enumerate(ind_xl_path),desc=\"Excel File Processed\",colour='red',total = len(ind_xl_path)):\n",
    "    ind_df = pd.read_excel(xlpth1,sheet_name='std')\n",
    "    total_df = pd.concat([total_df,ind_df],axis=0)\n",
    "total_df = total_df.reset_index(drop=True)\n",
    "save_path = r'\\\\shelter\\Kyu\\skin_aging\\clue_cohort\\wsi\\hovernet_out\\df\\output_excel\\total_df'\n",
    "total_df.to_excel(os.path.join(save_path, \"total_std\" + \".xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Then manually copy paste these to the overall clue data df!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
