{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Code from create_poc_dataset.ipynb:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-01T17:04:05.456076Z",
     "end_time": "2023-06-01T17:04:05.772518Z"
    }
   },
   "outputs": [],
   "source": [
    "### Code from create_poc_dataset.ipynb, exactly the same workflow, except that we now leave one out for test and use all the dataset for training. Note that after picking OTS_14684_6 as entirety, should pick only the tiles with compositions not in ECM, Fat, and White are chosen (from the excel sheet).\n",
    "### Main dataset, selecting all images but 1 WSI to train the US2mask segmentation model. Create the US-mask pair dataset below and create the train and test df to be used in training/inference:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calculate_tissue_composition(mask_image, num_classes=12):\n",
    "    total_pixels = mask_image.size\n",
    "    composition = np.zeros(num_classes)\n",
    "\n",
    "    for label in range(1, num_classes + 1):\n",
    "        mask = np.array(mask_image == label, dtype=np.uint8)\n",
    "        label_pixels = np.sum(mask)\n",
    "        composition[label - 1] = label_pixels / total_pixels\n",
    "    composition = np.round(composition, 3)\n",
    "    composition_freq = (composition > 0).astype('int')\n",
    "    return composition, composition_freq\n",
    "\n",
    "\n",
    "def create_train_test_df(train_mask_src_list, train_US_src_list):\n",
    "    \"\"\"\n",
    "    Assumes train_mask_src and train_US_src split is known b/w train and test, and they must be both equal lists of the filepaths to the mask and the US images.\n",
    "    \"\"\"\n",
    "    # initialize/create empty_df with column names:\n",
    "    all_df = pd.DataFrame(columns=[\"id\", \"wsi_name\", \"image_path\", \"mask_path\", \"composition\", \"composition_freq\"])\n",
    "\n",
    "    for src_idx in tqdm(range(len(train_mask_src_list)), colour='red', desc='WSI Processed'):\n",
    "        train_df = pd.DataFrame(columns=[\"id\", \"wsi_name\", \"image_path\", \"mask_path\"])  # reinitilize every WSI\n",
    "        train_df = train_df.reindex(range(len(train_mask_src_list)))\n",
    "        train_mask_src = train_mask_src_list[src_idx]\n",
    "        train_US_src = train_US_src_list[src_idx]\n",
    "        train_masklist = [os.path.join(train_mask_src, x) for x in os.listdir(train_mask_src)]\n",
    "        train_masklist = [x for x in train_masklist if x.endswith(\".png\")]\n",
    "        train_USlist = [os.path.join(train_US_src, x) for x in os.listdir(train_US_src)]\n",
    "        train_USlist = [x for x in train_USlist if x.endswith(\".png\")]\n",
    "        if len(train_USlist) != len(train_masklist):\n",
    "            print(\"Recheck the mask and US pair, number of files in one of the pairs is not equal for {} and {}\".format(\n",
    "                train_US_src, train_mask_src))\n",
    "        id_list, wsi_name_list, image_path_list, mask_path_list = [], [], [], []  # reinitialize every new WSI\n",
    "        for img_idx in tqdm(range(len(train_masklist)), colour='red', desc=\"Masks Processed per WSI\"):\n",
    "            masksrc = train_masklist[img_idx]\n",
    "            imgsrc = train_USlist[img_idx]\n",
    "            mask_img = np.array(Image.open(masksrc))\n",
    "            composition, composition_freq = calculate_tissue_composition(mask_img)\n",
    "            id = masksrc.split(\"\\\\\")[-1].split(\".png\")[0]\n",
    "            wsi_name = masksrc.split(\"\\\\\")[-2]\n",
    "            image_path = imgsrc\n",
    "            mask_path = masksrc\n",
    "            id_list.append(id)\n",
    "            wsi_name_list.append(wsi_name)\n",
    "            image_path_list.append(image_path)\n",
    "            mask_path_list.append(mask_path)\n",
    "            composition = np.array2string(composition)\n",
    "            composition_freq = np.array2string(composition_freq)\n",
    "            train_df.loc[img_idx, \"composition\"] = composition\n",
    "            train_df.loc[img_idx, \"composition_freq\"] = composition_freq\n",
    "        train_df[\"id\"] = id_list\n",
    "        train_df[\"wsi_name\"] = wsi_name_list\n",
    "        train_df[\"image_path\"] = image_path_list\n",
    "        train_df[\"mask_path\"] = mask_path_list\n",
    "        all_df = pd.concat([all_df, train_df], axis=0)\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "masksrc = r\"\\\\shelter\\Kyu\\unstain2mask\\masks\"\n",
    "USsrc = r\"\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_tiles\\US\"\n",
    "allmasksrc = [os.path.join(masksrc, x) for x in os.listdir(masksrc)]\n",
    "allUSsrc = [os.path.join(USsrc, x) for x in os.listdir(masksrc)]\n",
    "allUSsrc\n",
    "# # Let's just choose everything but OTS_14684_3 (that will be test data)\n",
    "del (allmasksrc[2])\n",
    "del (allUSsrc[2])\n",
    "poc_train_df = create_train_test_df(allmasksrc, allUSsrc)\n",
    "poc_train_df\n",
    "dst_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\"\n",
    "poc_train_df.to_excel(os.path.join(dst_src, \"train_df.xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do the same for inference to create test_df:\n",
    "masksrc = r\"\\\\shelter\\Kyu\\unstain2mask\\masks\"\n",
    "USsrc = r\"\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_tiles\\US\"\n",
    "allmasksrc = [os.path.join(masksrc, x) for x in os.listdir(masksrc)]\n",
    "allUSsrc = [os.path.join(USsrc, x) for x in os.listdir(masksrc)]\n",
    "# Let's just choose OTS_14684_3!\n",
    "poc_masksrc = allmasksrc[2]\n",
    "poc_USsrc = allUSsrc[2]\n",
    "poc_test_df = create_train_test_df([poc_masksrc], [poc_USsrc])\n",
    "poc_test_df\n",
    "dst_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\"\n",
    "poc_test_df.to_excel(os.path.join(dst_src, \"test_df.xlsx\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now edit train_df so that except for OTS_14684_3, the rest 4 of the WSIs don't sample ECM, Fat, and Whitespace tiles.\n",
    "dst_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\"\n",
    "saved_train_df_src = os.path.join(dst_src, \"train_df.xlsx\")\n",
    "saved_train_df = pd.read_excel(saved_train_df_src)\n",
    "saved_train_df\n",
    "wsi_names = np.unique(saved_train_df[\"wsi_name\"])\n",
    "wsi_names_skip = list(wsi_names[0:2]) + list(\n",
    "    wsi_names[3:5])  # leave out OTS_14684_6 (sincve we will use all tiles of OTS_14684_6)\n",
    "wsi_names_skip\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an empty dataframe with the desired columns\n",
    "new_train_df = pd.DataFrame(columns=[\"id\", \"wsi_name\", \"image_path\", \"mask_path\", \"composition\", \"composition_freq\"])\n",
    "\n",
    "# Iterate over the rows of saved_train_df\n",
    "for idx, row in tqdm(saved_train_df.iterrows(), total=saved_train_df.shape[0]):\n",
    "    if \"OTS_14684_6\" in row[\"wsi_name\"]:\n",
    "        # Don't edit rows with wsi_name \"OTS_14684_6\", simply append them to the new dataframe\n",
    "        new_train_df = new_train_df.append(row, ignore_index=True)\n",
    "    else:\n",
    "        string_array = row[\"composition\"]\n",
    "        pattern = r'(\\d+\\.\\d+|\\d+)'  # Regular expression pattern to match floating-point numbers\n",
    "        matches = re.findall(pattern, string_array)\n",
    "        numpy_array = np.array([float(x) for x in matches])\n",
    "        if np.sum(numpy_array[9:12]) > 0.7:\n",
    "            continue\n",
    "        else:\n",
    "            new_train_df = new_train_df.append(row, ignore_index=True)\n",
    "\n",
    "new_train_df\n",
    "dst_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\"\n",
    "saved_train_df_src = os.path.join(dst_src, \"new_train_df.xlsx\")\n",
    "new_train_df.to_excel(saved_train_df_src)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
