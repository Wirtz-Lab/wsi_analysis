{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:21:36.789679Z",
     "end_time": "2023-05-02T15:21:43.836130Z"
    }
   },
   "outputs": [],
   "source": [
    "#import necessary modules:\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if cuda/gpu available:\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                imagepath  label\n0       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n1       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n2       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n3       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n4       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n...                                                   ...    ...\n274642  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274643  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274644  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274645  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274646  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n\n[274647 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imagepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>274642</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274643</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274644</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274645</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274646</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>274647 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define dataset and dataloaders\n",
    "train_df_src = r'\\\\fatherserverdw\\Kevin\\unstained_blank_classifier\\train_df.xlsx'\n",
    "train_df = pd.read_excel(train_df_src) # 1= white , 0=nonwhite, unbalanced, 79271 0's and 195376 1's. Need stratifiedgroupKfold for CV.\n",
    "train_df = train_df.drop(columns=\"Unnamed: 0\")\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:21:43.836130Z",
     "end_time": "2023-05-02T15:21:58.171134Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First find mean and std of dataset for image normalization:\n",
    "### code for finding dataset std and mean from: https://kozodoi.me/blog/20210308/compute-image-stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "find_mean_std_dataset = False # already found it\n",
    "\n",
    "if find_mean_std_dataset:\n",
    "    class Unstain2StainData(Dataset):\n",
    "        def __init__(self,df,transform=None):\n",
    "            self.df = df\n",
    "            self.directory = df[\"imagepath\"].tolist()\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return int(len(self.directory)/3)\n",
    "\n",
    "        def __getitem__(self,idx):\n",
    "            path = self.directory[idx]\n",
    "            image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image = image)['image']\n",
    "            return image\n",
    "\n",
    "    device      = torch.device('cpu')\n",
    "    num_workers = 0\n",
    "    image_size  = 384\n",
    "    batch_size  = 4\n",
    "\n",
    "    augmentations = A.Compose([A.Resize(height= image_size ,width = image_size ),\n",
    "                                       A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "                                       ToTensorV2()])\n",
    "\n",
    "    unstain2stain_dataset = Unstain2StainData(df = train_df, transform = augmentations)# data loader\n",
    "    image_loader = DataLoader(unstain2stain_dataset,\n",
    "                              batch_size  = batch_size,\n",
    "                              shuffle     = False,\n",
    "                              num_workers = num_workers,\n",
    "                              pin_memory  = True)\n",
    "\n",
    "    images = next(iter(image_loader))\n",
    "    print(\"Images have a tensor size of {}.\".\n",
    "          format(images.size()))\n",
    "    # compute mean/std for 1/3 of the images for time's sake:\n",
    "    # placeholders\n",
    "    psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "    psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "    # loop through images\n",
    "    for inputs in tqdm(image_loader,colour='red'):\n",
    "        psum    += inputs.sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "        psum_sq += (inputs ** 2).sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "\n",
    "    # pixel count\n",
    "    count = len(train_df) * image_size * image_size\n",
    "\n",
    "    # mean and std\n",
    "    total_mean = psum / count\n",
    "    total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "    total_std  = torch.sqrt(total_var)\n",
    "\n",
    "    # output\n",
    "    print('mean: ' + str(total_mean))\n",
    "    print('std:  ' + str(total_std))\n",
    "\n",
    "    #mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:24.851767Z",
     "end_time": "2023-05-02T15:48:24.883020Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We can now use the above calculated mean and std value for our augmentations for the dataset. Also, use StratifiedKfold to perform 5-fold CV with each fold containing the equal percentages of the blank and non-blank images:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# add stratifiedkfold to df:\n",
    "new_df_train = train_df.copy(deep=True)\n",
    "strat_kfold = StratifiedKFold(shuffle = True, random_state = 42) #use default n_split = 5, random_state for reproducibility\n",
    "\n",
    "#split on white and non-white and add a new column fold to it:\n",
    "for each_fold, (idx1,idx2) in enumerate (strat_kfold.split(X = new_df_train, y = new_df_train['label'])):\n",
    "    new_df_train.loc[idx2,'fold'] = int(each_fold) #create new fold column with the fold number (up to 5)\n",
    "\n",
    "new_df_train[\"fold\"] = new_df_train[\"fold\"].apply(lambda x: int(x)) # somehow doesn't turn to int, so change to int, fold from 0~4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                imagepath  label  fold\n0       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n1       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     4\n2       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n3       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     0\n4       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n...                                                   ...    ...   ...\n274642  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     3\n274643  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     2\n274644  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     4\n274645  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     1\n274646  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     4\n\n[274647 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imagepath</th>\n      <th>label</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>274642</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>274643</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>274644</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>274645</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274646</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>274647 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "fold  label\n0     0        15855\n      1        39075\n1     0        15854\n      1        39076\n2     0        15854\n      1        39075\n3     0        15854\n      1        39075\n4     0        15854\n      1        39075\nName: fold, dtype: int64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratios of the folds are: [0.40575815738963533, 0.4057221824137578, 0.4057325655790147, 0.4057325655790147, 0.4057325655790147]\n"
     ]
    }
   ],
   "source": [
    "#check if stratification worked by grouping:\n",
    "grouped = new_df_train.groupby(['fold','label']) # look how it's splitted\n",
    "display(grouped.fold.count())\n",
    "\n",
    "ratio_list = []\n",
    "for k in range(5):\n",
    "    ratio = grouped.fold.count()[k][0]/grouped.fold.count()[k][1]\n",
    "    ratio_list.append(ratio)\n",
    "print(\"the ratios of the folds are: {}\".format(ratio_list)) #ratios to check stratification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As we can see above, stratification was successful. Now define transforms:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#define transforms/image augmentation for the dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384), # efficientnetv2_s 384 x 384\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]) #calculated above mean & std\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    " # validate at 1024 x 1024, you want to use val dataset to real world application, but maybe resize to 384 if performance is bad.\n",
    "    #transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]) #calculated above mean & std\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# build train dataset\n",
    "class TrainDataSet(Dataset):\n",
    "    # initialize df, label, imagepath and transforms\n",
    "    def __init__(self, df, label=True, transforms = None):\n",
    "        self.df = df\n",
    "        self.label = df[\"label\"].tolist()\n",
    "        self.imagepaths = df[\"imagepath\"].tolist()\n",
    "        self.transforms = transforms\n",
    "    # define length, which is simply length of all imagepaths\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    # define main function to read image and label, apply transform function and return the transformed images.\n",
    "    def __getitem__(self,idx):\n",
    "        image_path = self.imagepaths[idx]\n",
    "        img = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        if self.label:\n",
    "            label = self.label[idx]\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# all model configs go here so that they can be changed when we want to:\n",
    "class model_config:\n",
    "    seed = 42\n",
    "    model_name = \"efficientnetv2_l\"\n",
    "    train_batch_size = 16\n",
    "    valid_batch_size = 32\n",
    "    epochs = 5\n",
    "    learning_rate = 0.001\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    T_max = int(30000/train_batch_size*epochs) # for cosineannealingLR, explore different values\n",
    "    weight_decay = 1e-6 # explore different weight decay (Adam optimizer)\n",
    "    n_accumulate = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    iters_to_accumulate = max(1,32//train_batch_size) # for scaling accumulated gradients\n",
    "    eta_min = 1e-5\n",
    "    model_save_directory = os.path.join(os.getcwd(),\"model\") #assuming os.getcwd is the wsi_analysis directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# sets the seed of the entire notebook so results are the same every time we run for reproducibility. no randomness, everything is controlled.\n",
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # when running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(model_config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# define dataloading function:\n",
    "def load_dataset(fold):\n",
    "    model_df_train = new_df_train.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "    model_df_val = new_df_train.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    train_dataset = TrainDataSet(df = model_df_train, transforms = train_transform)\n",
    "    val_dataset = TrainDataSet(df = model_df_val, transforms = val_transform)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset,\n",
    "        batch_size = model_config.train_batch_size, # pin_memory= true allows faster data transport from cpu to gpu\n",
    "        num_workers = 0, pin_memory = True, shuffle = True)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset,\n",
    "        batch_size = model_config.valid_batch_size,\n",
    "        num_workers = 0, pin_memory = True, shuffle = True)\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([16, 3, 384, 384]), and Labels have a tensor size of torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = load_dataset(fold = 0)\n",
    "image, labels = next(iter(train_dataloader))\n",
    "print(\"Images have a tensor size of {}, and Labels have a tensor size of {}\".\n",
    "      format(image.size(),labels.size()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([32, 3, 1024, 1024]), and Labels have a tensor size of torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(val_dataloader))\n",
    "print(\"Images have a tensor size of {}, and Labels have a tensor size of {}\".\n",
    "      format(images.size(),labels.size()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now move on to the model and loss function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = timm.create_model(model_config.model_name,pretrained=False)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_features,1) #in_features = 1280, out_features = 1, so that 0 or 1 binary classification\n",
    "    # model.add_module('sigmoid', nn.Sigmoid()) # obtain probability b/w 0 and 1\n",
    "    model.to(model_config.device) # model to gpu\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def loss_func(y_pred,y_true):\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    return loss(y_pred,y_true)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def return_f1_score(y_true,y_pred):\n",
    "    # y_true = y_true.to(torch.int)\n",
    "    # y_pred = y_pred.to(torch.int)\n",
    "    f_one_score = f1_score(y_true,y_pred)\n",
    "    return f_one_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Loop:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#from: https://pytorch.org/docs/stable/notes/amp_examples.html in \"working with scaled gradients\"\n",
    "def epoch_train(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train() # set mode to train\n",
    "    dataset_size = 0 #initialize\n",
    "    running_loss = 0.0 #initialize\n",
    "    scaler = GradScaler() # enable GradScaler\n",
    "    pbar = tqdm(enumerate(dataloader), total = len(dataloader), desc='Train')\n",
    "    for idx, (images, labels) in pbar:\n",
    "        images = images.to(device, dtype=torch.float) # move tensor to gpu\n",
    "        labels  = labels.to(device, dtype=torch.float) # move tensor to gpu\n",
    "        batch_size = images.size(0) # return batch size (N*C*H*W), or N (64)\n",
    "\n",
    "        with autocast(enabled=True,dtype=torch.float16): # enable autocast for forward pass\n",
    "            y_pred = model(images) # forward pass, get y_pred from input\n",
    "            labels = labels.unsqueeze(1) #make tensor size from [16] to [16,1] for label, same as images\n",
    "            loss   = loss_func(y_pred, labels) # compute losses from y_pred\n",
    "            loss   = loss / model_config.iters_to_accumulate # need to normalize since accumulating gradients\n",
    "        scaler.scale(loss).backward() # accumulates the scaled gradients\n",
    "\n",
    "        if (idx + 1) % model_config.iters_to_accumulate == 0: # scale updates should only happen at batch granularity\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # update scale for next iteration\n",
    "            optimizer.zero_grad() # zero the accumulated scaled gradients\n",
    "            scheduler.step() # change lr,make sure to call this after scaler.step\n",
    "\n",
    "        running_loss += (loss.item() * batch_size) # update current running loss for all images in batch\n",
    "        dataset_size += batch_size # update current datasize\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size # get current epoch average loss\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n",
    "                        lr=f'{current_lr:0.5f}')\n",
    "\n",
    "    torch.cuda.empty_cache() #clear gpu memory after every epoch\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss #return loss for this epoch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Loop:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "@torch.no_grad() # disable gradient calc for validation\n",
    "def epoch_valid(model, dataloader, device, epoch):\n",
    "    model.eval() # set mode to eval\n",
    "    dataset_size = 0 #initialize\n",
    "    running_loss = 0.0 #initialize\n",
    "    valid_score_history = [] #keep validation score\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Validation')\n",
    "    for idx, (images, labels) in pbar:\n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "        labels   = labels.to(device, dtype=torch.float)\n",
    "        batch_size = images.size(0)\n",
    "        y_pred  = model(images)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        loss    = loss_func(y_pred, labels)\n",
    "\n",
    "        running_loss += (loss.item() * batch_size) #update current running loss\n",
    "        dataset_size += batch_size #update current datasize\n",
    "        epoch_loss = running_loss / dataset_size #divide epoch loss by current datasize\n",
    "\n",
    "        y_pred = nn.Sigmoid()(y_pred) #sigmoid for binary classification\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "        y_pred = np.round(y_pred)\n",
    "        f_one_score = return_f1_score(labels,y_pred) #fetch f1 score\n",
    "        valid_score_history.append(f_one_score)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_postfix(valid_loss=f'{epoch_loss:0.3f}',\n",
    "                        lr=f'{current_lr:0.4f}')\n",
    "    valid_score_history = np.mean(valid_score_history, axis=0)\n",
    "    torch.cuda.empty_cache() #clear gpu memory after every epoch\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss, valid_score_history #return loss and valid_score_history for this epoch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "\n",
    "    start = time.time() # measure time\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) #deepcopy\n",
    "    best_f1      = 0 # initial best score\n",
    "    best_epoch     = -1 # initial best epoch\n",
    "    history = defaultdict(list) # history defaultdict to store relevant variables\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "        print(f'Epoch {epoch}/{num_epochs}', end='')\n",
    "        train_loss = epoch_train(model, optimizer, scheduler,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           device=model_config.device, epoch=epoch)\n",
    "        valid_loss, valid_score_history = epoch_valid(model, val_dataloader,\n",
    "                                                 device=model_config.device,\n",
    "                                                 epoch=epoch)\n",
    "        valid_f1 = valid_score_history\n",
    "        history['Train Loss'].append(train_loss)\n",
    "        history['Valid Loss'].append(valid_loss)\n",
    "        history['Valid Dice'].append(valid_f1)\n",
    "\n",
    "        print(f'Valid Dice: {valid_f1:0.4f}')\n",
    "\n",
    "        # if dice score improves, save the best model\n",
    "        if valid_f1 >= best_f1:\n",
    "            print(f\"Validation F1-Score Improved ({best_f1:0.4f} ---> {valid_f1:0.4f})\")\n",
    "            best_f1    = valid_f1\n",
    "            best_epoch   = epoch\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = os.path.join(model_config.model_save_directory,f\"best_epoch-{fold:02d}.pt\")\n",
    "            if not os.path.exists(model_config.model_save_directory):\n",
    "                os.makedirs(model_config.model_save_directory)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(\"Model Saved!\")\n",
    "\n",
    "        # save the most recent model\n",
    "        latest_model_wts = copy.deepcopy(model.state_dict())\n",
    "        PATH = os.path.join(model_config.model_save_directory,f\"latest_epoch-{fold:02d}.pt\")\n",
    "        if not os.path.exists(model_config.model_save_directory):\n",
    "            os.makedirs(model_config.model_save_directory)\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60))\n",
    "    print(\"Best F1-Score: {:.4f}\".format(best_f1))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now build model using functions and train:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=model_config.learning_rate,\n",
    "                       weight_decay = model_config.weight_decay ) # default learning rate\n",
    "if model_config == \"CosineAnnealingLR\": # change to CosineAnnealingLR\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = model_config.T_max,\n",
    "                                               eta_min =  model_config.eta_min)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run Training!\n",
    "for fold in range(4):\n",
    "    print(f'Fold: {fold}')\n",
    "    train_dataloader, valid_dataloader = load_dataset(fold = fold)\n",
    "    model     = build_model()\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=model_config.learning_rate,\n",
    "                           weight_decay=model_config.weight_decay) # default learning rate\n",
    "\n",
    "    if model_config.scheduler == \"CosineAnnealingLR\": # change to CosineAnnealingLR\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                   T_max = model_config.T_max,\n",
    "                                                   eta_min =  model_config.eta_min)\n",
    "\n",
    "    model, history = run_training(model, optimizer, scheduler,\n",
    "                                  device=model_config.device,\n",
    "                                  num_epochs=model_config.epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
