{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import necessary modules:\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import lr_scheduler\n",
    "import timm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    #initialize transforms, the df, and directory for image and id\n",
    "    def __init__(self, df, transforms = None):\n",
    "        self.df = df\n",
    "        self.imagepaths = df[\"imagepath\"].tolist()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self): # upperbound of idx (index), so how many images we have\n",
    "        return len(self.df) # length of df is how many images we have\n",
    "\n",
    "    def __getitem__(self,idx): # index = each row of df\n",
    "        # read image:\n",
    "        image_path = self.imagepaths[idx]\n",
    "        img = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "        return torch.tensor(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# all model configs go here so that they can be changed when we want to:\n",
    "# Make sure this model_config is same as the train model_config when running inference:\n",
    "class model_config:\n",
    "    seed = 42\n",
    "    model_name = \"efficientnetv2_l\"\n",
    "    train_batch_size = 16\n",
    "    valid_batch_size = 32\n",
    "    epochs = 5\n",
    "    learning_rate = 0.001\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    T_max = int(30000/train_batch_size*epochs) # for cosineannealingLR, explore different values\n",
    "    weight_decay = 1e-6 # explore different weight decay (Adam optimizer)\n",
    "    n_accumulate = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    iters_to_accumulate = max(1,32//train_batch_size) # for scaling accumulated gradients\n",
    "    eta_min = 1e-5\n",
    "    model_save_directory = os.path.join(os.getcwd(),\"model\") #assuming os.getcwd is the wsi_analysis directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# sets the seed of the entire notebook so results are the same every time we run for reproducibility. no randomness, everything is controlled.\n",
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # when running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(model_config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def return_f1_score(y_true,y_pred):\n",
    "    f_one_score = f1_score(y_true,y_pred)\n",
    "    return f_one_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def loss_func(y_pred,y_true):\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    return loss(y_pred,y_true)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = timm.create_model(model_config.model_name,pretrained=False)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_features,1) #in_features = 1280, out_features = 1, so that 0 or 1 binary classification\n",
    "    # model.add_module('sigmoid', nn.Sigmoid()) # obtain probability b/w 0 and 1\n",
    "    model.to(model_config.device) # model to gpu\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    " # validate at 1024 x 1024, you want to use val dataset to real world application, but maybe resize to 384 if performance is bad.\n",
    "    #transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]) #calculated above mean & std\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(model_paths, test_loader, thr= 0.5):\n",
    "    pred_classes = []\n",
    "    labels = []\n",
    "    images = []\n",
    "\n",
    "    for idx, image in enumerate(tqdm(test_loader, total=len(test_loader), desc='Inference')):\n",
    "        image = image.to(model_config.device, dtype=torch.float) # .squeeze(0)\n",
    "        label = []\n",
    "        for path in model_paths:\n",
    "            model = build_model() # get the backbone architecture\n",
    "            model.load_state_dict(torch.load(path)) # get the weights saved from training\n",
    "            model.eval() # set model in eval mode, not train\n",
    "            output = model(image)\n",
    "            output = nn.Sigmoid()(output) # probabilities\n",
    "            label += output / len(model_paths) # prediction label is ensembled by using all of the models in the directory. Try to compare this and only using the one or two best models.\n",
    "        print(label)\n",
    "        label = (label>thr).to(torch.uint8).cpu().detach().numpy() # thresholded, >0.5 is 1 and <0.5 is 0\n",
    "        pred_classes.extend(label) # pred_classes is therefore a numpy array of 0's and 1's as a prediction class.\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "    \"predicted\":pred_classes})\n",
    "    return pred_df, images, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# open test_df:\n",
    "test_df_src = r\"\\\\fatherserverdw\\Kevin\\unstained_blank_classifier\\test_df.xlsx\"\n",
    "test_df = pd.read_excel(test_df_src)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\labadmin\\AppData\\Local\\Temp\\ipykernel_1112\\2804195440.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image)\n",
      "Inference:   0%|          | 0/25 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.4997], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4989], device='cuda:0'), tensor([0.4998], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4991], device='cuda:0'), tensor([0.4994], device='cuda:0'), tensor([0.4998], device='cuda:0'), tensor([0.4999], device='cuda:0'), tensor([0.4999], device='cuda:0'), tensor([0.4993], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4999], device='cuda:0'), tensor([0.4999], device='cuda:0'), tensor([0.4998], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.5000], device='cuda:0'), tensor([0.5000], device='cuda:0'), tensor([0.5000], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4997], device='cuda:0'), tensor([0.4996], device='cuda:0'), tensor([0.4992], device='cuda:0'), tensor([0.4940], device='cuda:0'), tensor([0.4758], device='cuda:0'), tensor([0.4929], device='cuda:0'), tensor([0.4866], device='cuda:0'), tensor([0.4985], device='cuda:0'), tensor([0.4894], device='cuda:0'), tensor([0.4927], device='cuda:0'), tensor([0.4948], device='cuda:0'), tensor([0.4854], device='cuda:0'), tensor([0.4911], device='cuda:0'), tensor([0.4765], device='cuda:0'), tensor([0.4953], device='cuda:0'), tensor([0.4941], device='cuda:0'), tensor([0.4965], device='cuda:0'), tensor([0.4943], device='cuda:0'), tensor([0.4781], device='cuda:0'), tensor([0.4954], device='cuda:0'), tensor([0.4958], device='cuda:0'), tensor([0.4981], device='cuda:0'), tensor([0.4980], device='cuda:0'), tensor([0.4946], device='cuda:0'), tensor([0.4960], device='cuda:0'), tensor([0.4958], device='cuda:0'), tensor([0.4884], device='cuda:0'), tensor([0.4954], device='cuda:0'), tensor([0.4995], device='cuda:0'), tensor([0.4966], device='cuda:0'), tensor([0.4944], device='cuda:0'), tensor([0.4973], device='cuda:0'), tensor([0.4962], device='cuda:0'), tensor([0.4946], device='cuda:0'), tensor([0.4853], device='cuda:0'), tensor([0.4858], device='cuda:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m saved_model_path \u001B[38;5;241m=\u001B[39m model_config\u001B[38;5;241m.\u001B[39mmodel_save_directory\n\u001B[0;32m      5\u001B[0m model_paths  \u001B[38;5;241m=\u001B[39m glob(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msaved_model_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/best_epoch*.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m pred_df, images, labels \u001B[38;5;241m=\u001B[39m \u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\wsi_analysis\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[15], line 18\u001B[0m, in \u001B[0;36minfer\u001B[1;34m(model_paths, test_loader, thr)\u001B[0m\n\u001B[0;32m     16\u001B[0m         label \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m output \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(model_paths) \u001B[38;5;66;03m# prediction label is ensembled by using all of the models in the directory. Try to compare this and only using the one or two best models.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(label)\n\u001B[1;32m---> 18\u001B[0m     label \u001B[38;5;241m=\u001B[39m (\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43mthr\u001B[49m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39muint8)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy() \u001B[38;5;66;03m# thresholded, >0.5 is 1 and <0.5 is 0\u001B[39;00m\n\u001B[0;32m     19\u001B[0m     pred_classes\u001B[38;5;241m.\u001B[39mextend(label) \u001B[38;5;66;03m# pred_classes is therefore a numpy array of 0's and 1's as a prediction class.\u001B[39;00m\n\u001B[0;32m     21\u001B[0m pred_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m\"\u001B[39m:pred_classes})\n",
      "\u001B[1;31mTypeError\u001B[0m: '>' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataSet(test_df, transforms = val_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=model_config.valid_batch_size,\n",
    "                          num_workers=0)\n",
    "saved_model_path = model_config.model_save_directory\n",
    "model_paths  = glob(f'{saved_model_path}/best_epoch*.pt')\n",
    "pred_df, images, labels = infer(model_paths, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['C:\\\\Users\\\\labadmin\\\\PycharmProjects\\\\wsi_analysis\\\\kevin\\\\binary_classifier\\\\model\\\\best_epoch-03.pt',\n 'C:\\\\Users\\\\labadmin\\\\PycharmProjects\\\\wsi_analysis\\\\kevin\\\\binary_classifier\\\\model\\\\best_epoch-04.pt']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
