{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:21:36.789679Z",
     "end_time": "2023-05-02T15:21:43.836130Z"
    }
   },
   "outputs": [],
   "source": [
    "#import necessary modules:\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                imagepath  label\n0       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n1       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n2       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n3       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n4       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n...                                                   ...    ...\n274642  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274643  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274644  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274645  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274646  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n\n[274647 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imagepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>274642</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274643</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274644</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274645</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274646</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>274647 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define dataset and dataloaders\n",
    "train_df_src = r'\\\\fatherserverdw\\Kevin\\unstained_blank_classifier\\train_df.xlsx'\n",
    "train_df = pd.read_excel(train_df_src) # 1= white , 0=nonwhite, unbalanced, 79271 0's and 195376 1's. Need stratifiedgroupKfold for CV.\n",
    "train_df = train_df.drop(columns=\"Unnamed: 0\")\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:21:43.836130Z",
     "end_time": "2023-05-02T15:21:58.171134Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First find mean and std of dataset for image normalization:\n",
    "### code for finding dataset std and mean from: https://kozodoi.me/blog/20210308/compute-image-stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Unstain2StainData(Dataset):\n",
    "    def __init__(self,df,transform=None):\n",
    "        self.df = df\n",
    "        self.directory = df[\"imagepath\"].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.directory)/3)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        path = self.directory[idx]\n",
    "        image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image = image)['image']\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:24.851767Z",
     "end_time": "2023-05-02T15:48:24.883020Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device      = torch.device('cpu')\n",
    "num_workers = 0\n",
    "image_size  = 384\n",
    "batch_size  = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:33.008241Z",
     "end_time": "2023-05-02T15:48:33.022678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "augmentations = A.Compose([A.Resize(height= image_size ,width = image_size ),\n",
    "                                   A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "                                   ToTensorV2()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:33.351994Z",
     "end_time": "2023-05-02T15:48:33.367925Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([4, 3, 384, 384]).\n"
     ]
    }
   ],
   "source": [
    "unstain2stain_dataset = Unstain2StainData(df = train_df, transform = augmentations)# data loader\n",
    "image_loader = DataLoader(unstain2stain_dataset,\n",
    "                          batch_size  = batch_size,\n",
    "                          shuffle     = False,\n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory  = True)\n",
    "images = next(iter(image_loader))\n",
    "print(\"Images have a tensor size of {}.\".\n",
    "      format(images.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:33.758043Z",
     "end_time": "2023-05-02T15:48:34.679812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from pathos.multiprocessing import ProcessingPool as Pool\n",
    "#\n",
    "# # placeholders\n",
    "# psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "# psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "#\n",
    "# def compute_sum_sq(inputs):\n",
    "#     return inputs.sum(axis=[0,2,3]), (inputs**2).sum(axis=[0,2,3])\n",
    "#\n",
    "# # create a pool of workers\n",
    "# pool = Pool()\n",
    "#\n",
    "# # loop through images and compute sums in parallel\n",
    "# results = pool.imap(compute_sum_sq, image_loader)\n",
    "#\n",
    "# # accumulate results\n",
    "# for psum_i, psum_sq_i in tqdm(results, total=len(image_loader), colour='red'):\n",
    "#     psum += psum_i\n",
    "#     psum_sq += psum_sq_i\n",
    "#\n",
    "# # close the pool\n",
    "# pool.close()\n",
    "#\n",
    "# # pixel count\n",
    "# count = len(train_df) * image_size * image_size\n",
    "#\n",
    "# # mean and std\n",
    "# total_mean = psum / count\n",
    "# total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "# total_std  = torch.sqrt(total_var)\n",
    "#\n",
    "# # output\n",
    "# print('mean: ' + str(total_mean))\n",
    "# print('std:  ' + str(total_std))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:21:58.499281Z",
     "end_time": "2023-05-02T15:21:58.546159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# # code without multiprocessing\n",
    "# # compute mean/std for 1/3 of the images for time's sake:\n",
    "# # placeholders\n",
    "# psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "# psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "#\n",
    "# # loop through images\n",
    "# for inputs in tqdm(image_loader,colour='red'):\n",
    "#     psum    += inputs.sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "#     psum_sq += (inputs ** 2).sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "#\n",
    "# # pixel count\n",
    "# count = len(train_df) * image_size * image_size\n",
    "#\n",
    "# # mean and std\n",
    "# total_mean = psum / count\n",
    "# total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "# total_std  = torch.sqrt(total_var)\n",
    "#\n",
    "# # output\n",
    "# print('mean: ' + str(total_mean))\n",
    "# print('std:  ' + str(total_std))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T15:48:38.149283Z",
     "end_time": "2023-05-02T21:59:40.508252Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We can now use the above calculated mean and std value for our augmentations for the dataset. Also, use StratifiedKfold to perform 5-fold CV with each fold containing the equal percentages of the blank and non-blank images:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# add stratifiedkfold to df:\n",
    "new_df_train = train_df.copy(deep=True)\n",
    "strat_kfold = StratifiedKFold(shuffle = True, random_state = 42) #use default n_split = 5, random_state for reproducibility\n",
    "\n",
    "#split on white and non-white and add a new column fold to it:\n",
    "for each_fold, (idx1,idx2) in enumerate (strat_kfold.split(X = new_df_train, y = new_df_train['label'])):\n",
    "    new_df_train.loc[idx2,'fold'] = int(each_fold) #create new fold column with the fold number (up to 5)\n",
    "\n",
    "new_df_train[\"fold\"] = new_df_train[\"fold\"].apply(lambda x: int(x)) # somehow doesn't turn to int, so change to int, fold from 0~4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                imagepath  label  fold\n0       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n1       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     4\n2       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n3       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     0\n4       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0     1\n...                                                   ...    ...   ...\n274642  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     3\n274643  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     2\n274644  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     4\n274645  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     1\n274646  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1     4\n\n[274647 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imagepath</th>\n      <th>label</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>274642</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>274643</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>274644</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>274645</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274646</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>274647 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "fold  label\n0     0        15855\n      1        39075\n1     0        15854\n      1        39076\n2     0        15854\n      1        39075\n3     0        15854\n      1        39075\n4     0        15854\n      1        39075\nName: fold, dtype: int64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratios of the folds are: [0.40575815738963533, 0.4057221824137578, 0.4057325655790147, 0.4057325655790147, 0.4057325655790147]\n"
     ]
    }
   ],
   "source": [
    "#check if stratification worked by grouping:\n",
    "grouped = new_df_train.groupby(['fold','label']) # look how it's splitted\n",
    "display(grouped.fold.count())\n",
    "\n",
    "ratio_list = []\n",
    "for k in range(5):\n",
    "    ratio = grouped.fold.count()[k][0]/grouped.fold.count()[k][1]\n",
    "    ratio_list.append(ratio)\n",
    "print(\"the ratios of the folds are: {}\".format(ratio_list)) #ratios to check stratification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As we can see above, stratification was successful. Now define transforms:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#define transforms/image augmentation for the dataset\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384), # efficientnetv2_s 384 x 384\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]) #calculated above mean & std\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    " # validate at 1024 x 1024, you want to use val dataset to real world application, but maybe resize to 384 if performance is bad.\n",
    "    #transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2966, 0.3003, 0.3049], std=[0.4215, 0.4267, 0.4332]) #calculated above mean & std\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# build train dataset\n",
    "class TrainDataSet(Dataset):\n",
    "    # initialize df, label, imagepath and transforms\n",
    "    def __init__(self, df, label=True, transforms = None):\n",
    "        self.df = df\n",
    "        self.label = df[\"label\"].tolist()\n",
    "        self.imagepaths = df[\"imagepath\"].tolist()\n",
    "        self.transforms = transforms\n",
    "    # define length, which is simply length of all imagepaths\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    # define main function to read image and label, apply transform function and return the transformed images.\n",
    "    def __getitem__(self,idx):\n",
    "        image_path = self.imagepaths[idx]\n",
    "        img = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        if self.label:\n",
    "            label = self.label[idx]\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# all model configs go here so that they can be changed when we want to:\n",
    "class model_config:\n",
    "    seed = 42\n",
    "    model_name = \"efficientnetv2_l\"\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 8\n",
    "    epochs = 5\n",
    "    learning_rate = 0.001\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    n_accumulate = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Sets the seed of the entire notebook so results are the same every time we run for reproducibility.\n",
    "def set_seed(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(model_config.seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# define dataloading function:\n",
    "def load_dataset(fold):\n",
    "    model_df_train = new_df_train.query(\"fold!=@fold\").reset_index(drop=True)\n",
    "    model_df_val = new_df_train.query(\"fold==@fold\").reset_index(drop=True)\n",
    "    train_dataset = TrainDataSet(df = model_df_train, transforms = train_transform)\n",
    "    val_dataset = TrainDataSet(df = model_df_val, transforms = val_transform)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset,\n",
    "        batch_size = model_config.train_batch_size, # pin_memory= true allows faster data transport from cpu to gpu\n",
    "        num_workers = 0, pin_memory = True, shuffle = True)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset,\n",
    "        batch_size = model_config.valid_batch_size,\n",
    "        num_workers = 0, pin_memory = True, shuffle = True)\n",
    "    return train_dataloader, val_dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([4, 3, 384, 384]), and Labels have a tensor size of torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = load_dataset(fold = 0)\n",
    "image, labels = next(iter(train_dataloader))\n",
    "print(\"Images have a tensor size of {}, and Labels have a tensor size of {}\".\n",
    "      format(image.size(),labels.size()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([8, 3, 1024, 1024]), and Labels have a tensor size of torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(val_dataloader))\n",
    "print(\"Images have a tensor size of {}, and Labels have a tensor size of {}\".\n",
    "      format(images.size(),labels.size()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now move on to the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = timm.create_model(model_config.model_name,pretrained=False)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_features,1) #in_features = 1280, out_features = 1, so that 0 or 1 binary classification\n",
    "    model.add_module('sigmoid', nn.Sigmoid()) # obtain probability b/w 0 and 1\n",
    "    model.to(model_config.device) # model to gpu\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #define loss function, optimizer and device\n",
    "\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### training loop:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, criterion):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'outputs/model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_plots(train_accuracy_list, val_accuracy_list,train_loss_list,val_loss_list):    # accuracy plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_accuracy_list, color='green', linestyle='-',\n",
    "        label='train accuracy'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_accuracy_list, color='blue', linestyle='-',\n",
    "        label='validation accuracy'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/accuracy.png')\n",
    "\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss_list, color='orange', linestyle='-',\n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_loss_list, color='red', linestyle='-',\n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/loss_vs_epochs.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#training loop\n",
    "num_epochs = 10\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_accuracy_list, val_accuracy_list = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item()\n",
    "            val_correct +=  (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "save_model(epoch, model, optimizer, criterion)\n",
    "save_plots(train_accuracy, val_accuracy,train_loss,val_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
