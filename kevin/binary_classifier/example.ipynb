{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:42:58.196762Z",
     "end_time": "2023-05-01T17:42:58.228644Z"
    }
   },
   "outputs": [],
   "source": [
    "#import necessary modules:\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.datasets as datasets\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                imagepath  label\n0       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n1       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n2       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n3       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n4       \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      0\n...                                                   ...    ...\n274642  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274643  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274644  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274645  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n274646  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...      1\n\n[274647 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imagepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>274642</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274643</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274644</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274645</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>274646</th>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>274647 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define dataset and dataloaders\n",
    "train_df_src = r'\\\\fatherserverdw\\Kevin\\unstained_blank_classifier\\train_df.xlsx'\n",
    "train_df = pd.read_excel(train_df_src) # 1= white , 0=nonwhite, unbalanced, 79271 0's and 195376 1's. Need stratifiedgroupKfold for CV.\n",
    "train_df = train_df.drop(columns=\"Unnamed: 0\")\n",
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:43:49.721480Z",
     "end_time": "2023-05-01T17:44:01.683941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# first find mean and std of dataset for image normalization:\n",
    "class Unstain2StainData(Dataset):\n",
    "    def __init__(self,df,transform=None):\n",
    "        self.df = df\n",
    "        self.directory = df[\"imagepath\"].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.directory)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        path = self.directory[idx]\n",
    "        image = cv2.imread(path, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image = image)['image']\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:44:05.402716Z",
     "end_time": "2023-05-01T17:44:05.412747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "device      = torch.device('cpu')\n",
    "num_workers = 0\n",
    "image_size  = 384\n",
    "batch_size  = 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:44:05.668520Z",
     "end_time": "2023-05-01T17:44:05.688305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "augmentations = A.Compose([A.Resize(height= image_size ,width = image_size ),\n",
    "                                   A.Normalize(mean=(0,0,0), std=(1,1,1)),\n",
    "                                   ToTensorV2()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:44:07.761742Z",
     "end_time": "2023-05-01T17:44:07.779012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([8, 3, 384, 384]).\n"
     ]
    }
   ],
   "source": [
    "unstain2stain_dataset = Unstain2StainData(df = train_df, transform = augmentations)# data loader\n",
    "image_loader = DataLoader(unstain2stain_dataset,\n",
    "                          batch_size  = batch_size,\n",
    "                          shuffle     = False,\n",
    "                          num_workers = num_workers,\n",
    "                          pin_memory  = True)\n",
    "images = next(iter(image_loader))\n",
    "print(\"Images have a tensor size of {}.\".\n",
    "      format(images.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T17:44:09.335026Z",
     "end_time": "2023-05-01T17:44:10.900927Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|\u001B[31m███████▋  \u001B[0m| 26239/34331 [17:53:40<5:07:13,  2.28s/it] "
     ]
    }
   ],
   "source": [
    "# compute mean/std:\n",
    "# placeholders\n",
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs in tqdm(image_loader,colour='red'):\n",
    "    psum    += inputs.sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3]) # sum over axis 1\n",
    "\n",
    "# pixel count\n",
    "count = len(train_df) * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: ' + str(total_mean))\n",
    "print('std:  ' + str(total_std))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add stratifiedkfold to df:\n",
    "new_df_train = train_df.deepcopy()\n",
    "strat_kfold = StratifiedKFold(shuffle = True, random_state = 42) #use default n_split = 5, random_state for reproducibility\n",
    "\n",
    "#split (stratification) on is_empty and group (groupkfold) on case_number:\n",
    "for each_fold, (idx1,idx2) in enumerate (strat_kfold.split(X = new_df_train, y = new_df_train['label'])):\n",
    "    new_df_train.loc[idx2,'fold'] = int(each_fold) #create new fold column with the fold number (up to 5)\n",
    "\n",
    "#check if stratification worked by grouping:\n",
    "grouped = new_df_train.groupby(['fold','label']) # look how it's splitted\n",
    "display(grouped.id.count())\n",
    "\n",
    "ratio_list = []\n",
    "for k in range(5):\n",
    "    ratio = grouped.id.count()[k][0]/grouped.id.count()[k][1]\n",
    "    ratio_list.append(ratio)\n",
    "print(\"the ratios of the folds are: {}\".format(ratio_list)) #ratios to check stratification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define transforms/image augmentation for the dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(384), # efficientnetv2_s 384 x 384\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #imagenet1k weights\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    " # validate at 1024 x 1024, you want to use val dataset to real world application, but maybe resize to 384 if performance is bad.\n",
    "    # transforms.Resize(384),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #imagenet1k weights\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build train and valid dataset\n",
    "class TrainDataSet(Dataset):\n",
    "    # initialize df, label, imagepath and transforms\n",
    "    def __init__(self, df, label=True, transform = None):\n",
    "        self.df = df\n",
    "        self.label = df[\"label\"].tolist()\n",
    "        self.imagepaths = df[\"imagepath\"].tolist()\n",
    "        self.transform = transform\n",
    "    # define length, which is simply length of all imagepaths\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    # define main function to read image and label, apply transform function and return the transformed images.\n",
    "    def __getitem__(self,idx):\n",
    "        image_path = self.imagepaths[idx]\n",
    "        image = cv2.imread(image_path, -1)\n",
    "        if self.label:\n",
    "            label = self.label[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #define pre-trained resNet-18 model\n",
    "# model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 2)  #replace/edit output layer with a new linear layer for binary classification- blank or not blank\n",
    "\n",
    "# use efficientnetv2 small instead, should do better than resnet18/50\n",
    "model = EfficientNet.from_pretrained('efficientnetv2-s')\n",
    "\n",
    "# Modify the last layer to output a single binary classification output\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, 1)\n",
    "\n",
    "#define loss function, optimizer and device\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### training loop:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model(epoch, model, optimizer, criterion):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'outputs/model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_plots(train_accuracy_list, val_accuracy_list,train_loss_list,val_loss_list):    # accuracy plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_accuracy_list, color='green', linestyle='-',\n",
    "        label='train accuracy'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_accuracy_list, color='blue', linestyle='-',\n",
    "        label='validation accuracy'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/accuracy.png')\n",
    "\n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss_list, color='orange', linestyle='-',\n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        val_loss_list, color='red', linestyle='-',\n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/loss_vs_epochs.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#training loop\n",
    "num_epochs = 10\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_accuracy_list, val_accuracy_list = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item()\n",
    "            val_correct +=  (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "save_model(epoch, model, optimizer, criterion)\n",
    "save_plots(train_accuracy, val_accuracy,train_loss,val_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
