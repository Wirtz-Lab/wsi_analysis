{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Define transforms for the dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m all_ns_path \u001B[38;5;241m=\u001B[39m natsorted(all_ns_path)\n\u001B[0;32m      4\u001B[0m non_empty_images_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_excel(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mshelter\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mKyu\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124munstain2stain\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtiles\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mregistered_tiles\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mHE\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcomposition_per_tile\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mns_train_image_path.xlsx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m non_empty_images_df\u001B[38;5;241m.\u001B[39miloc[:,\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mnon_empty_images_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr\u001B[49m\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistrated\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregistered\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m non_empty_images_df\n",
      "File \u001B[1;32m~\\.conda\\envs\\wsi_analysis1\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5895\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5896\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   5897\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   5898\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   5899\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   5900\u001B[0m ):\n\u001B[0;32m   5901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 5902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\wsi_analysis1\\lib\\site-packages\\pandas\\core\\accessor.py:182\u001B[0m, in \u001B[0;36mCachedAccessor.__get__\u001B[1;34m(self, obj, cls)\u001B[0m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001B[39;00m\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessor\n\u001B[1;32m--> 182\u001B[0m accessor_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001B[39;00m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;66;03m# NDFrame\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__setattr__\u001B[39m(obj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name, accessor_obj)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wsi_analysis1\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:181\u001B[0m, in \u001B[0;36mStringMethods.__init__\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01marrays\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstring_\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StringDtype\n\u001B[1;32m--> 181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_categorical \u001B[38;5;241m=\u001B[39m is_categorical_dtype(data\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(data\u001B[38;5;241m.\u001B[39mdtype, StringDtype)\n",
      "File \u001B[1;32m~\\.conda\\envs\\wsi_analysis1\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:235\u001B[0m, in \u001B[0;36mStringMethods._validate\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    232\u001B[0m inferred_dtype \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39minfer_dtype(values, skipna\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inferred_dtype \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m allowed_types:\n\u001B[1;32m--> 235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan only use .str accessor with string values!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inferred_dtype\n",
      "\u001B[1;31mAttributeError\u001B[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "nspath = r'\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_tiles\\US'\n",
    "all_ns_path = glob(os.path.join(nspath,'*','*.png')) # list of all images\n",
    "all_ns_path = natsorted(all_ns_path)\n",
    "non_empty_images_df = pd.read_excel(r\"\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_tiles\\HE\\composition_per_tile\\ns_train_image_path.xlsx\")\n",
    "non_empty_images_df = non_empty_images_df.drop('Unnamed: 0.1',axis=1)\n",
    "non_empty_images_df.iloc[:,1] = non_empty_images_df.iloc[:,1].str.replace('registrated', 'registered')\n",
    "non_empty_images_df\n",
    "# non_empty_images = list(non_empty_images_df.iloc[:,1])\n",
    "# non_empty_images = natsorted(non_empty_images)\n",
    "# empty_images = set(all_ns_path) - set (non_empty_images)\n",
    "# print(len(empty_images))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0\n0               0\n1               1\n2               2\n3               3\n4               4\n...           ...\n79266       79266\n79267       79267\n79268       79268\n79269       79269\n79270       79270\n\n[79271 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79266</th>\n      <td>79266</td>\n    </tr>\n    <tr>\n      <th>79267</th>\n      <td>79267</td>\n    </tr>\n    <tr>\n      <th>79268</th>\n      <td>79268</td>\n    </tr>\n    <tr>\n      <th>79269</th>\n      <td>79269</td>\n    </tr>\n    <tr>\n      <th>79270</th>\n      <td>79270</td>\n    </tr>\n  </tbody>\n</table>\n<p>79271 rows Ã— 1 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "non_empty_images_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "non_empty_images_df.to_excel(r\"\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_tiles\\HE\\composition_per_tile\\ns_train_image_path.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define dataset and dataloaders\n",
    "train_dataset = ImageFolder(root='path/to/train/folder', transform=train_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder(root='path/to/val/folder', transform=val_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #define pre-trained resNet-18 model\n",
    "# model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 2)  #replace/edit output layer with a new linear layer for binary classification- blank or not blank\n",
    "model = EfficientNet.from_pretrained('efficientnetv2-s')\n",
    "\n",
    "# Modify the last layer to output a single binary classification output\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, 1)\n",
    "\n",
    "#define loss function, optimizer and device\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
